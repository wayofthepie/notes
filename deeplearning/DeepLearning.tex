
\documentclass[12pt,reqno]{book}      % Sets 12pt font, equation numbers on right
\usepackage{amsmath,amssymb,amsthm,amsfonts} % Typical maths resource packages
\usepackage{graphics}                 % Packages to allow inclusion of graphics
\usepackage{color}                    % For creating coloured text and background
\usepackage[margin=1in]{geometry}

\usepackage[colorlinks,citecolor=blue,linkcolor=blue]{hyperref}                 % For creating hyperlinks in cross references. It should be after the color package. The option colorlinks produces colored entries without boxes. The option citecolor=blue changes the default green citations to blue.

\usepackage{pgfplots}
\usepackage[mathscr]{euscript}

\DeclareGraphicsExtensions{.pdf}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}{Definition}


\def\R{\mathbb{ R}}
\def\S{\mathbb{ S}}
\def\I{\mathbb{ I}}
\makeindex


\title{A \LaTeX \ Book Skeleton  }

\author{\htmladdnormallink           % Puts a hyperlink on to the author's name
{Stephen O'Brien}{https://github.com/wayofthepie}
{\small\em \copyright 2017 }}

\date{ }
    
\begin{document}
\maketitle
\addcontentsline{toc}{chapter}{Contents}
\pagenumbering{roman}
\tableofcontents
% \listoffigures
% \listoftables

\chapter{Neural Networks}
\section{Logistic Regression}
\subsection{Basics}
If $x$ is a picture we want to predict the probability ($\hat{y}$) that this is a picture of some specific type, e.g a cat picture. More formally - given $x \in \mathbb{R}^{n_{x}}$, $w \in \mathbb{R}^{n_{x}}$ and 
$b \in \mathbb{R}$, we want $\hat{y} = P(y=1 | x)$ (probability of $y = 1$ given $x$). How do we get $\hat{y}$?

We could try the following linear function: $\hat{y} = w^\intercal x + b$. But this isn't a good algorithm for binary classification as we want the output, $\hat{y}$ to be $0 \leq \hat{y} \leq 1$ as $w^\intercal x + b$ can be much bigger than $1$ or even negative!

Let $z \in \mathbb{R}$,  $z = w^\intercal x + b$, therefore in Logistic regression our output instead will be:

\begin{align}
\sigma(z) = \dfrac{1}{1 + e^{-z}}
\end{align}

Note that $\sigma$ is a sigmoid function (in this case a \textit{logistic curve}), who's characteristic shape is given below:

\begin{center}
\begin{tikzpicture}
    \begin{axis}
    [
        grid=major,     
        xmin=-6,
        xmax=6,
        axis x line=bottom,
        ytick={0,.5,1},
        ymax=1,
        axis y line=middle,
        ytick={0,0.5},
        xticklabels={,,},
        xlabel={z}
    ]
        \addplot
        [
            blue,
            mark=none,
            samples=100,
            domain=-6:6,
        ]
        (x,{1/(1+exp(-x))});
    \end{axis}
\end{tikzpicture}
\end{center}

Given the above, if
\begin{itemize}
	\item $z$ is large then $\sigma(z) \approx \dfrac{1}{1 + 0} = 1$
    \item $z$ is a large negative number then $\sigma(z) = \dfrac{1}{1 + e^{-z}} \approx \dfrac{1}{1 + \text{Bignum}} \approx 0$
\end{itemize}

So we get
\begin{align}
\hat{y} = \sigma(w^\intercal x + b) \text{, where } \sigma(z) = \dfrac{1}{1 + e^{-z}}
\end{align}


\subsection{Cost Function}
Given $\big( x^{(1)}, y^{(1)} \big),\dots{} , \big( x^{(m)}, y^{(m)} \big)$, we want $\hat{y}^{(i)} \approx y^{(i)}$, where $m$ is the training set size. The \textit{loss/error function} $\mathscr{L}$ is need to measure how good our output $\hat{y}$ is when the true label is $y$.  For logistic regression using the squared error  \textit{loss function} will not work well, as it has many local optima. The squared error function is 

\begin{align}
\mathscr{L}(\hat{y}, y) = \dfrac{1}{2} (\hat{y} - y)^{2}
\end{align}

So what we use in logistic regression for $\mathscr{L}$ is the following

\begin{align}
\mathscr{L}(\hat{y}, y) = - \big( y \log \hat{y} + (1 - y) \log  (1 - \hat{y}) \big)
\end{align}

\begin{itemize}
	\item if $y = 1$ : $\mathscr{L}(\hat{y}, y) = - \log \hat{y} \leftarrow$ want $\log \hat{y}$ to be large, want $\hat{y}$ to be large.
	\item if $ y= 0$ : $\mathscr{L}(\hat{y}, y) = - \log (1 - \hat{y}) \leftarrow$ want $\log (1 - \hat{y})$ to be large, want $\hat{y}$ to be small.
\end{itemize}

The \textit{loss function} is defined with respect to a single training example measuring how you are doing on this single training example. The \textit{cost function}, $\mathscr{J}$, measures how you are doing on the \textit{entire} training set - i.e. the cost of the parameters. 

\begin{align}
\mathscr{J}(w,b) &= \dfrac{1}{m} \sum_{i = 1}^{m} \mathscr{L} \big( \hat{y}^{(i)}, y^{(i)} \big) \\
&= - \dfrac{1}{m} \sum_{i = 1}^{m} y \log \hat{y} + (1 - y) \log  (1 - \hat{y})
\end{align}

% End of book

\pagestyle{headings}
\pagenumbering{arabic}

\include{ch1}
\include{ch2}

\begin{thebibliography}{99}
  \addcontentsline{toc}{chapter}{Bibliography}
\bibitem{lamport} L. Lamport. {\bf \LaTeX \ A Document Preparation System}
Addison-Wesley, California 1986.
\end{thebibliography}

\include{index}
 \addcontentsline{toc}{chapter}{Index}
\end{document}
